package gujaratiSerachEngine.Crawler.CuteCrawler;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Set;


public class Spider
{
	private static final long MAX_PAGES_TO_SEARCH = 30000;
	private Set<String> pagesVisited = new HashSet<String>();
	private List<String> pagesToVisit = new LinkedList<String>();  
	private int count=0;

	public List<String> getPagesToVisit() {
		return pagesToVisit;
	}
	static void initLogFile(){
		try{
		 File file = new File(logFile);
		 
			// if file doesnt exists, then create it
			if (file.exists()) {
				file.delete();
			}
			file.createNewFile();
		}
		catch(Exception e){
			e.printStackTrace();
		}
	}
	  /**
	   * Our main launching point for the Spider's functionality. Internally it creates spider legs
	   * that make an HTTP request and parse the response (the web page).
	   * 
	   * @param url
	   *            - The starting point of the spider
	   * @param searchWord
	   *            - The word or string that you are searching for
	   */
	  
	  static void writeToFile(String HTMLData,String path) {
		  try{
			  File file = new File("/home/alfa/Crawled/"+path+".html");
				 
				// if file doesnt exists, then create it
				if (!file.exists()) {
					file.createNewFile();
				}
				 
				FileWriter fw = new FileWriter(file.getAbsoluteFile());
				BufferedWriter bw = new BufferedWriter(fw);
				bw.write(HTMLData);
				bw.close();
		  }catch(Exception x){
			  x.printStackTrace();
		  }
	  }

	  static void logLink(String HTML_Link,String path) {
		  try{
			  File file = new File("/home/alfa/Crawled/links.log");
				 
				// if file doesnt exists, then create it
				if (!file.exists()) {
					file.createNewFile();
				}
				 
				FileWriter fw = new FileWriter(file.getAbsoluteFile(),true);
				BufferedWriter bw = new BufferedWriter(fw);
				bw.write(HTML_Link+">"+path+".html\n");
				bw.close();
		  }catch(Exception x){
			  System.out.println(x);
		  }
	  }
	  
	  /**
	   * Returns the next URL to visit (in the order that they were found). We also do a check to make
	   * sure this method doesn't return a URL that has already been visited.
	   * 
	   * @return
	   */
	  private String nextUrl()
	  {
	      String nextUrl;
	      do
	      {
	          nextUrl = this.pagesToVisit.remove(0);
	      } while(this.pagesVisited.contains(nextUrl));
	      this.pagesVisited.add(nextUrl);
	      return nextUrl;
	  }
	  public void addUrl(String url){
		  this.pagesToVisit.add(url);
	  }
	  
	public void startCrawl() {	  {
	      while(this.pagesVisited.size() < MAX_PAGES_TO_SEARCH){
	          String currentUrl =null;;
	          SpiderLeg leg = new SpiderLeg();
	          if(this.pagesToVisit.isEmpty()){
	              System.out.println("All Pages Visited");
	          }
	          else{
	              currentUrl = this.nextUrl();
	          }
	          //leg.crawl(currentUrl); // Lots of stuff happening here. Look at the crawl method in
	          //System.out.println(leg.crawl(currentUrl));// SpiderLeg
	          String data = null;
	          if(currentUrl!=null  && !currentUrl.isEmpty())
	        	  data=leg.crawl(currentUrl);
	          if(data!=null){
	        	  logLink(currentUrl,count+"");
		          writeToFile(data,count+"");
		          System.out.println("Done "+count);
		          count++;
	          }
	          this.pagesToVisit.addAll(leg.getLinks());
	      }
	      System.out.println("\n**Done** Visited " + this.pagesVisited.size() + " web page(s)");
	}
}
}
